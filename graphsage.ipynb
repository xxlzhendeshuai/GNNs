{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import argparse\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCenter(object):\n",
    "    \"\"\"加载数据集\n",
    "\tParameter:\n",
    "\t\tfile_paths:{数据文件存放地址1,数据文件存放地址2}\n",
    "\t\"\"\"\n",
    "    def __init__(self,file_paths):\n",
    "        \"\"\"file_paths:{name:root,...,}\"\"\"\n",
    "        super(DataCenter,self).__init__()\n",
    "        self.file_paths = file_paths\n",
    "    \n",
    "    def load_Dataset(self,dataset='cora'):\n",
    "        \"\"\"读取存放在指定路径的数据集\"\"\"\n",
    "        feat_list = [] # 用于存放每个节点特征向量的列表\n",
    "        label_list = [] # 用于存放每个节点对应类别的列表\n",
    "        node_map = {} # 将节点进行重新编码\n",
    "        label_map = {} # 将label映射为数字\n",
    "        \n",
    "        if dataset == 'cora':\n",
    "            content = self.file_paths['cora_content'] # 获取cora_content的地址\n",
    "            cite = self.file_paths['cora_cite'] # 获取cora_cite的地址\n",
    "            with open(content) as f1:\n",
    "                for i,each_sample in enumerate(f1.readlines()): # 遍历每个样本的特征\n",
    "                    sample_clean = each_sample.strip().split()\n",
    "                    # 提取每个样本的特征，其中第一个元素和最后一个元素是样本名称和对应的标签\n",
    "                    feat_list.append(sample_clean[1:-1])\n",
    "                    # 把节点名称映射为节点编号 \n",
    "                    node_map[sample_clean[0]]=i\n",
    "                    label = sample_clean[-1]\n",
    "                    if label not in label_map.keys():\n",
    "                    \t# 把label转化为数字\n",
    "                        label_map[label] = len(label_map)\n",
    "                    label_list.append(label_map[label])\n",
    "                feat_list = np.asarray(feat_list,dtype=np.float64)\n",
    "                label_list = np.asarray(label_list,dtype=np.int64)\n",
    "            \n",
    "            # 获得每个节点的邻居{v0:[v0的邻居集合],v1:[v1的邻居集合]}\n",
    "            adj_lists = defaultdict(set)\n",
    "            with open(cite) as f2:\n",
    "                for j,each_pair in enumerate(f2.readlines()):\n",
    "                    pair = each_pair.strip().split()\n",
    "                    assert len(pair) == 2\n",
    "                    adj_lists[node_map[pair[0]]].add(node_map[pair[1]])\n",
    "                    adj_lists[node_map[pair[1]]].add(node_map[pair[0]])\n",
    "            \n",
    "            assert len(feat_list) == len(label_list) == len(adj_lists)\n",
    "            train_index,test_index,val_index = self._split_data(feat_list.shape[0])\n",
    "            # 使用getattr()可以获得数据\n",
    "            setattr(self,dataset+'_test',test_index)\n",
    "            setattr(self,dataset+'_val',val_index)\n",
    "            setattr(self,dataset+'_train',train_index)\n",
    "            setattr(self,dataset+'_feats',feat_list)\n",
    "            setattr(self,dataset+'_labels',label_list)\n",
    "            setattr(self,dataset+'_adj_lists',adj_lists)\n",
    "        \n",
    "    def _split_data(self,number_of_nodes,test_split=3,val_split=6):\n",
    "    \t# 打乱顺序\n",
    "        rand_indices = np.random.permutation(number_of_nodes)\n",
    "        test_size = number_of_nodes // test_split\n",
    "        val_size = number_of_nodes // val_split\n",
    "        test_index = rand_indices[:test_size]\n",
    "        val_index = rand_indices[test_size:test_size+val_size]\n",
    "        train_index = rand_indices[test_size+val_size:]\n",
    "        return train_index,test_index,val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedLoss(object):\n",
    "\t\"\"\"docstring for UnsupervisedLoss\"\"\"\n",
    "\tdef __init__(self, adj_lists, train_nodes, device):\n",
    "\t\tsuper(UnsupervisedLoss, self).__init__()\n",
    "\t\tself.Q = 10 # 负样本的数量\n",
    "\t\tself.N_WALKS = 6 # 每个节点随机游走的次数\n",
    "\t\tself.WALK_LEN = 1 # 每次随机游走的步长\n",
    "\t\tself.N_WALK_LEN = 5 # 每次负样本随机游走几个节点\n",
    "\t\tself.MARGIN = 3 \n",
    "\t\tself.adj_lists = adj_lists #{v0:[v0的邻居集合],v1:[v1的邻居集合],...,vn:[vn的邻居集合]}\n",
    "\t\tself.train_nodes = train_nodes # 训练节点\n",
    "\t\tself.device = device # cpu or gpu\n",
    "\n",
    "\t\tself.target_nodes = None\n",
    "\t\tself.positive_pairs = [] # 存放正例样本 [(v0,v0邻居中采样到的正例节点),....,]\n",
    "\t\tself.negtive_pairs = [] # 存放负例样本 [(v0,v0邻居中采样到的负例节点),....,]\n",
    "\t\tself.node_positive_pairs = {} # {v0:[(v0,从v0开始随机游走采样到的正例节点),(v0,从v0开始随机游走采样到的正例节点)],...,vn:[(vn,从vn开始随机游走采样到的正例节点)]}\n",
    "\t\tself.node_negtive_pairs = {} # {v0:[(v0,从v0开始随机游走采样到的负例节点),(v0,从v0开始随机游走采样到的负例节点)],...,vn:[(vn,从vn开始随机游走采样到的负例节点)]}\n",
    "\t\tself.unique_nodes_batch = [] # 一个batch所有会用到的节点及其邻居节点\n",
    "\n",
    "\tdef get_loss_sage(self, embeddings, nodes):\n",
    "\t\tassert len(embeddings) == len(self.unique_nodes_batch) #判断是不是每个节点都有了embeddings\n",
    "\t\tassert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))] # 判断目标节点集和unique集里的节点是否1一一对应\n",
    "\t\tnode2index = {n:i for i,n in enumerate(self.unique_nodes_batch)} # 把节点重新编码\n",
    "\n",
    "\t\tnodes_score = []\n",
    "\t\tassert len(self.node_positive_pairs) == len(self.node_negtive_pairs) # 确定正例节点对和负例节点对的数量是否相同\n",
    "\t\tfor node in self.node_positive_pairs: # 遍历所有节点\n",
    "\t\t\tpps = self.node_positive_pairs[node] # 获得对应的正例 [(v0,v0正例样本1),(v0,v0正例样本2),...,(v0,v0正例样本n)]\n",
    "\t\t\tnps = self.node_negtive_pairs[node] # 获得每个节点对应的负例 [(v0,v0负例样本1),(v0,v0负例样本2),...,(v0,v0负例样本n)]\n",
    "\t\t\tif len(pps) == 0 or len(nps) == 0: # 判断是否都有正例和负例\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# Q * Exception(negative score)计算负例样本的Loss，即Loss函数的后一项\n",
    "\t\t\tindexs = [list(x) for x in zip(*nps)] # [[源节点,...,源节点],[采样得到的负节点1,...,采样得到的负节点n]]\n",
    "\t\t\tnode_indexs = [node2index[x] for x in indexs[0]] # 获得源节点的编号\n",
    "\t\t\tneighb_indexs = [node2index[x] for x in indexs[1]] # 负样本节点的编号\n",
    "\t\t\tneg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs]) # 计算余弦相似性\n",
    "\t\t\tneg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0) # 计算损失的后一项\n",
    "\t\t\t#print(neg_score)\n",
    "\n",
    "\t\t\t# multiple positive score 计算正列样本的Loss，即Loss函数的前一项\n",
    "\t\t\tindexs = [list(x) for x in zip(*pps)]\n",
    "\t\t\tnode_indexs = [node2index[x] for x in indexs[0]]\n",
    "\t\t\tneighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "\t\t\tpos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "\t\t\tpos_score = torch.log(torch.sigmoid(pos_score)) # 计算损失的前一项\n",
    "\t\t\t#print(pos_score)\n",
    "\n",
    "\t\t\tnodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1)) # 把每个节点的损失加入到列表中\n",
    "\t\t\t\t\n",
    "\t\tloss = torch.mean(torch.cat(nodes_score, 0)) # 求平均\n",
    "\t\t\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef get_loss_margin(self, embeddings, nodes):\n",
    "\t\tassert len(embeddings) == len(self.unique_nodes_batch)\n",
    "\t\tassert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "\t\tnode2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "\t\tnodes_score = []\n",
    "\t\tassert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "\t\tfor node in self.node_positive_pairs:\n",
    "\t\t\tpps = self.node_positive_pairs[node]\n",
    "\t\t\tnps = self.node_negtive_pairs[node]\n",
    "\t\t\tif len(pps) == 0 or len(nps) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tindexs = [list(x) for x in zip(*pps)]\n",
    "\t\t\tnode_indexs = [node2index[x] for x in indexs[0]]\n",
    "\t\t\tneighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "\t\t\tpos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "\t\t\tpos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n",
    "\n",
    "\t\t\tindexs = [list(x) for x in zip(*nps)]\n",
    "\t\t\tnode_indexs = [node2index[x] for x in indexs[0]]\n",
    "\t\t\tneighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "\t\t\tneg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "\t\t\tneg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)\n",
    "\n",
    "\t\t\tnodes_score.append(torch.max(torch.tensor(0.0).to(self.device), neg_score-pos_score+self.MARGIN).view(1,-1))\n",
    "\t\t\t# nodes_score.append((-pos_score - neg_score).view(1,-1))\n",
    "\n",
    "\t\tloss = torch.mean(torch.cat(nodes_score, 0),0)\n",
    "\n",
    "\t\t# loss = -torch.log(torch.sigmoid(pos_score))-4*torch.log(torch.sigmoid(-neg_score))\n",
    "\t\t\n",
    "\t\treturn loss\n",
    "\n",
    "\n",
    "\tdef extend_nodes(self, nodes, num_neg=6):\n",
    "\t\t\"\"\"获得目标节点集的正样本和负样本，输出这些节点的集合\"\"\"\n",
    "\t\tself.positive_pairs = []\n",
    "\t\tself.node_positive_pairs = {}\n",
    "\t\tself.negtive_pairs = []\n",
    "\t\tself.node_negtive_pairs = {}\n",
    "\n",
    "\t\tself.target_nodes = nodes\n",
    "\t\tself.get_positive_nodes(nodes)\n",
    "\t\t# print(self.positive_pairs)\n",
    "\t\tself.get_negtive_nodes(nodes, num_neg) \n",
    "\t\t# print(self.negtive_pairs)\n",
    "\t\tself.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x]) | set([i for x in self.negtive_pairs for i in x]))\n",
    "\t\tassert set(self.target_nodes) < set(self.unique_nodes_batch)\n",
    "\t\treturn self.unique_nodes_batch\n",
    "\n",
    "\tdef get_positive_nodes(self, nodes):\n",
    "\t\treturn self._run_random_walks(nodes) # 通过随机游走获得正列样本\n",
    "\n",
    "\tdef get_negtive_nodes(self, nodes, num_neg):\n",
    "\t\tfor node in nodes: # 遍历每个节点\n",
    "\t\t\tneighbors = set([node])\n",
    "\t\t\tfrontier = set([node])\n",
    "\t\t\tfor i in range(self.N_WALK_LEN):\n",
    "\t\t\t\tcurrent = set() \n",
    "\t\t\t\tfor outer in frontier:\n",
    "\t\t\t\t\tcurrent |= self.adj_lists[int(outer)] #获取frontier中所有的邻居节点\n",
    "\t\t\t\tfrontier = current - neighbors #去除源节点\n",
    "\t\t\t\tneighbors |= current # 源节点+邻居节点\n",
    "\t\t\tfar_nodes = set(self.train_nodes) - neighbors # 减去train_nodes里源节点及其一阶邻居\n",
    "\t\t\tneg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes # 从二阶邻居开始采样\n",
    "\t\t\tself.negtive_pairs.extend([(node, neg_node) for neg_node in neg_samples])\n",
    "\t\t\tself.node_negtive_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n",
    "\t\treturn self.negtive_pairs\n",
    "\n",
    "\tdef _run_random_walks(self, nodes):\n",
    "\t\tfor node in nodes: # 遍历每个节点\n",
    "\t\t\tif len(self.adj_lists[int(node)]) == 0: # 若该节点没有邻居节点则跳过\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tcur_pairs = [] # 创建一个\n",
    "\t\t\tfor i in range(self.N_WALKS): # 每个节点会有N_WALKS次的随机游走\n",
    "\t\t\t\tcurr_node = node # \n",
    "\t\t\t\tfor j in range(self.WALK_LEN): # 每次随机游走走WALK_LEN的长度\n",
    "\t\t\t\t\tneighs = self.adj_lists[int(curr_node)]\n",
    "\t\t\t\t\tnext_node = random.choice(list(neighs))\n",
    "\t\t\t\t\t# self co-occurrences are useless\n",
    "\t\t\t\t\tif next_node != node and next_node in self.train_nodes:\n",
    "\t\t\t\t\t\tself.positive_pairs.append((node,next_node))\n",
    "\t\t\t\t\t\tcur_pairs.append((node,next_node))\n",
    "\t\t\t\t\tcurr_node = next_node\n",
    "\n",
    "\t\t\tself.node_positive_pairs[node] = cur_pairs\n",
    "\t\treturn self.positive_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    \"\"\"一个最简单的一层分类模型\n",
    "    Parameters:\n",
    "        input_size:输入维度\n",
    "        num_classes:类别数量\n",
    "    return:\n",
    "        logists:最大概率对应的标签\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size,num_classes):\n",
    "        super(Classification,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,num_classes) # 定义一个input_size*num_classes的线性层\n",
    "        self.init_params() # 初始化权重参数\n",
    "        \n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if len(param.size()) == 2: # 如果参数是矩阵的话就重新初始化\n",
    "                nn.init.xavier_uniform_(param) \n",
    "    \n",
    "    def forward(self,x):\n",
    "        logists = torch.log_softmax(self.fc1(x),1) # 利用log_softmax来获得最终输出的类别\n",
    "        return logists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageLayer(nn.Module):\n",
    "\t\"\"\"\n",
    "\t一层SageLayer\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_size, out_size, gcn=False): \n",
    "\t\tsuper(SageLayer, self).__init__()\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.gcn = gcn\n",
    "\t\tself.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gcn else 2 * self.input_size)) #初始化权重参数w*input.T\n",
    "\t\tself.init_params() # 调整权重参数分布\n",
    "\n",
    "\tdef init_params(self):\n",
    "\t\tfor param in self.parameters():\n",
    "\t\t\tnn.init.xavier_uniform_(param)\n",
    "\n",
    "\tdef forward(self, self_feats, aggregate_feats, neighs=None):\n",
    "\t\t\"\"\"\n",
    "\t\tParameters:\n",
    "\t\t\tself_feats:源节点的特征向量\n",
    "\t\t\taggregate_feats:聚合后的邻居节点特征\n",
    "\t\t\"\"\"\n",
    "\t\tif not self.gcn: # 如果不是gcn的话就要进行concatenate\n",
    "\t\t\tcombined = torch.cat([self_feats, aggregate_feats], dim=1)\n",
    "\t\telse:\n",
    "\t\t\tcombined = aggregate_feats\n",
    "\t\tcombined = F.relu(self.weight.mm(combined.t())).t()\n",
    "\t\treturn combined\n",
    "\n",
    "class GraphSage(nn.Module):\n",
    "\t\"\"\"定义一个GraphSage模型\"\"\"\n",
    "\tdef __init__(self, num_layers, input_size, out_size, raw_features, adj_lists, device, gcn=False, agg_func='MEAN'):\n",
    "\t\tsuper(GraphSage, self).__init__()\n",
    "\t\tself.input_size = input_size \n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.num_layers = num_layers # Graphsage的层数\n",
    "\t\tself.gcn = gcn\n",
    "\t\tself.device = device\n",
    "\t\tself.agg_func = agg_func\n",
    "\t\tself.raw_features = raw_features\n",
    "\t\tself.adj_lists = adj_lists\n",
    "\t\t# 定义每一层的输入和输出\n",
    "\t\tfor index in range(1, num_layers+1):\n",
    "\t\t\tlayer_size = out_size if index != 1 else input_size\n",
    "\t\t\tsetattr(self, 'sage_layer'+str(index), SageLayer(layer_size, out_size, gcn=self.gcn))#除了第1层的输入为input_size,其余层的输入和输出均为outsize\n",
    "\n",
    "\tdef forward(self, nodes_batch):\n",
    "\t\t\"\"\"\n",
    "\t\t为一批节点生成嵌入表示\n",
    "\t\tParameters:\n",
    "\t\t\tnodes_batch:目标批次的节点\n",
    "\t\t\"\"\"\n",
    "\t\tlower_layer_nodes = list(nodes_batch) # 初始化第一层节点\n",
    "\t\tnodes_batch_layers = [(lower_layer_nodes,)] # 存放每一层的节点信息\n",
    "\t\tfor i in range(self.num_layers):\n",
    "\t\t\tlower_samp_neighs, lower_layer_nodes_dict, lower_layer_nodes= self._get_unique_neighs_list(lower_layer_nodes) # 根据当前层节点获得下一层节点\n",
    "\t\t\tnodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))\n",
    "\n",
    "\t\tassert len(nodes_batch_layers) == self.num_layers + 1\n",
    "\n",
    "\t\tpre_hidden_embs = self.raw_features # 初始化h0\n",
    "\t\tfor index in range(1, self.num_layers+1):\n",
    "\t\t\tnb = nodes_batch_layers[index][0]  #所有邻居节点\n",
    "\t\t\tpre_neighs = nodes_batch_layers[index-1] # 上一层的邻居节点\n",
    "\t\t\taggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs)\n",
    "\t\t\tsage_layer = getattr(self, 'sage_layer'+str(index))\n",
    "\t\t\tif index > 1:\n",
    "\t\t\t\tnb = self._nodes_map(nb, pre_hidden_embs, pre_neighs)\n",
    "\t\t\t# self.dc.logger.info('sage_layer.')\n",
    "\t\t\tcur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb],\n",
    "\t\t\t\t\t\t\t\t\t\taggregate_feats=aggregate_feats)\n",
    "\t\t\tpre_hidden_embs = cur_hidden_embs\n",
    "\n",
    "\t\treturn pre_hidden_embs\n",
    "\n",
    "\tdef _nodes_map(self, nodes, hidden_embs, neighs):\n",
    "\t\tlayer_nodes, samp_neighs, layer_nodes_dict = neighs\n",
    "\t\tassert len(samp_neighs) == len(nodes)\n",
    "\t\tindex = [layer_nodes_dict[x] for x in nodes]\n",
    "\t\treturn index\n",
    "\n",
    "\tdef _get_unique_neighs_list(self, nodes, num_sample=10):\n",
    "\t\t_set = set \n",
    "\t\tto_neighs = [self.adj_lists[int(node)] for node in nodes] # 获取目标节点集的所有邻居节点[[v0的邻居],[v1的邻居],[v2的邻居]]\n",
    "\t\tif not num_sample is None: # 如果num_sample为实数的话\n",
    "\t\t\t_sample = random.sample  \n",
    "\t\t\tsamp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs] # [set(随机采样的邻居集合),set(),set()]\n",
    "            # 遍历所有邻居集合如果邻居节点数>=num_sample，就从邻居节点集中随机采样num_sample个邻居节点，否则直接把邻居节点集放进去\n",
    "\t\telse:\n",
    "\t\t\tsamp_neighs = to_neighs \n",
    "\t\tsamp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)] # 把源节点也放进去\n",
    "\t\t_unique_nodes_list = list(set.union(*samp_neighs)) #展平\n",
    "\t\ti = list(range(len(_unique_nodes_list))) # 重新编号\n",
    "\t\tunique_nodes = dict(list(zip(_unique_nodes_list, i)))\n",
    "\t\treturn samp_neighs, unique_nodes, _unique_nodes_list\n",
    "\n",
    "\tdef aggregate(self, nodes, pre_hidden_embs, pre_neighs, num_sample=10):\n",
    "\t\tunique_nodes_list, samp_neighs, unique_nodes = pre_neighs # 上一层的源节点，...,....,\n",
    "\t\tassert len(nodes) == len(samp_neighs) \n",
    "\t\tindicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))] # 判断每个节点是否出现在邻居节点中\n",
    "\t\tassert (False not in indicator)\n",
    "\t\tif not self.gcn:\n",
    "\t\t\t# 如果不适用gcn就要把源节点去除\n",
    "\t\t\tsamp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]\n",
    "\t\tif len(pre_hidden_embs) == len(unique_nodes):\n",
    "\t\t\tembed_matrix = pre_hidden_embs\n",
    "\t\telse:\n",
    "\t\t\tembed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)]\n",
    "\t\t# self.dc.logger.info('3')\n",
    "\t\tmask = torch.zeros(len(samp_neighs), len(unique_nodes))\n",
    "\t\tcolumn_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "\t\trow_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "\t\tmask[row_indices, column_indices] = 1 # 每个源节点为一行，一行元素中1对应的就是邻居节点的位置\n",
    "\n",
    "\t\tif self.agg_func == 'MEAN':\n",
    "\t\t\tnum_neigh = mask.sum(1, keepdim=True) # 计算每个源节点有多少个邻居节点\n",
    "\t\t\tmask = mask.div(num_neigh).to(embed_matrix.device) # \n",
    "\t\t\taggregate_feats = mask.mm(embed_matrix)\n",
    "\n",
    "\t\telif self.agg_func == 'MAX':\n",
    "\t\t\t# print(mask)\n",
    "\t\t\tindexs = [x.nonzero() for x in mask==1]\n",
    "\t\t\taggregate_feats = []\n",
    "\t\t\tfor feat in [embed_matrix[x.squeeze()] for x in indexs]:\n",
    "\t\t\t\tif len(feat.size()) == 1:\n",
    "\t\t\t\t\taggregate_feats.append(feat.view(1, -1))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taggregate_feats.append(torch.max(feat,0)[0].view(1, -1))\n",
    "\t\t\taggregate_feats = torch.cat(aggregate_feats, 0)\n",
    "\t\treturn aggregate_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataCenter, ds, graphSage, classification, device, max_vali_f1, name, cur_epoch):\n",
    "\ttest_nodes = getattr(dataCenter, ds+'_test') # 获得测试集\n",
    "\tval_nodes = getattr(dataCenter, ds+'_val') # 获得验证集\n",
    "\tlabels = getattr(dataCenter, ds+'_labels') # 获得标签\n",
    "\n",
    "\tmodels = [graphSage, classification] \n",
    "\n",
    "\tparams = [] # 将两个模型的参数存入一个列表中\n",
    "\tfor model in models:\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\tif param.requires_grad:\n",
    "\t\t\t\tparam.requires_grad = False\n",
    "\t\t\t\tparams.append(param)\n",
    "\n",
    "\tembs = graphSage(val_nodes)\n",
    "\tlogists = classification(embs)\n",
    "\t_, predicts = torch.max(logists, 1)\n",
    "\tlabels_val = labels[val_nodes]\n",
    "\tassert len(labels_val) == len(predicts)\n",
    "\tcomps = zip(labels_val, predicts.data)\n",
    "\n",
    "\tvali_f1 = f1_score(labels_val, predicts.cpu().data, average=\"micro\")\n",
    "\tprint(\"Validation F1:\", vali_f1)\n",
    "\n",
    "\tif vali_f1 > max_vali_f1:\n",
    "\t\tmax_vali_f1 = vali_f1\n",
    "\t\tembs = graphSage(test_nodes)\n",
    "\t\tlogists = classification(embs)\n",
    "\t\t_, predicts = torch.max(logists, 1)\n",
    "\t\tlabels_test = labels[test_nodes]\n",
    "\t\tassert len(labels_test) == len(predicts)\n",
    "\t\tcomps = zip(labels_test, predicts.data)\n",
    "\n",
    "\t\ttest_f1 = f1_score(labels_test, predicts.cpu().data, average=\"micro\")\n",
    "\t\tprint(\"Test F1:\", test_f1)\n",
    "\n",
    "\t\tfor param in params:\n",
    "\t\t\tparam.requires_grad = True\n",
    "\n",
    "\t\t# torch.save(models, './model_best_{}_ep{}_{:.4f}.torch'.format(name, cur_epoch, test_f1))\n",
    "\n",
    "\tfor param in params:\n",
    "\t\tparam.requires_grad = True\n",
    "\n",
    "\treturn max_vali_f1\n",
    "\n",
    "def get_gnn_embeddings(gnn_model, dataCenter, ds):\n",
    "    print('Loading embeddings from trained GraphSAGE model.')\n",
    "    features = np.zeros((len(getattr(dataCenter, ds+'_labels')), gnn_model.out_size))\n",
    "    nodes = np.arange(len(getattr(dataCenter, ds+'_labels'))).tolist()\n",
    "    b_sz = 500\n",
    "    batches = math.ceil(len(nodes) / b_sz)\n",
    "    embs = []\n",
    "    for index in range(batches):\n",
    "        nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n",
    "        embs_batch = gnn_model(nodes_batch)\n",
    "        assert len(embs_batch) == len(nodes_batch)\n",
    "        embs.append(embs_batch)\n",
    "        # if ((index+1)*b_sz) % 10000 == 0:\n",
    "        #     print(f'Dealed Nodes [{(index+1)*b_sz}/{len(nodes)}]')\n",
    "\n",
    "    assert len(embs) == batches\n",
    "    embs = torch.cat(embs, 0)\n",
    "    assert len(embs) == len(nodes)\n",
    "    print('Embeddings loaded.')\n",
    "    return embs.detach()\n",
    "\n",
    "def train_classification(dataCenter, graphSage, classification, ds, device, max_vali_f1, name, epochs=800):\n",
    "\t\"\"\"训练分类器\"\"\"\n",
    "\tprint('Training Classification ...')\n",
    "\tc_optimizer = torch.optim.SGD(classification.parameters(), lr=0.5)\n",
    "\t# train classification, detached from the current graph\n",
    "\t#classification.init_params()\n",
    "\tb_sz = 50\n",
    "\ttrain_nodes = getattr(dataCenter, ds+'_train')\n",
    "\tlabels = getattr(dataCenter, ds+'_labels')\n",
    "\tfeatures = get_gnn_embeddings(graphSage, dataCenter, ds)\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttrain_nodes = shuffle(train_nodes)\n",
    "\t\tbatches = math.ceil(len(train_nodes) / b_sz)\n",
    "\t\tvisited_nodes = set()\n",
    "\t\tfor index in range(batches):\n",
    "\t\t\tnodes_batch = train_nodes[index*b_sz:(index+1)*b_sz]\n",
    "\t\t\tvisited_nodes |= set(nodes_batch)\n",
    "\t\t\tlabels_batch = labels[nodes_batch]\n",
    "\t\t\tembs_batch = features[nodes_batch]\n",
    "\n",
    "\t\t\tlogists = classification(embs_batch)\n",
    "\t\t\tloss = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "\t\t\tloss /= len(nodes_batch)\n",
    "\t\t\t# print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Dealed Nodes [{}/{}] '.format(epoch+1, epochs, index, batches, loss.item(), len(visited_nodes), len(train_nodes)))\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\t\n",
    "\t\t\tnn.utils.clip_grad_norm_(classification.parameters(), 5)\n",
    "\t\t\tc_optimizer.step()\n",
    "\t\t\tc_optimizer.zero_grad()\n",
    "\n",
    "\t\tmax_vali_f1 = evaluate(dataCenter, ds, graphSage, classification, device, max_vali_f1, name, epoch)\n",
    "\treturn classification, max_vali_f1\n",
    "\n",
    "def apply_model(dataCenter, ds, graphSage, classification, unsupervised_loss, b_sz, unsup_loss, device, learn_method):\n",
    "\ttest_nodes = getattr(dataCenter, ds+'_test')\n",
    "\tval_nodes = getattr(dataCenter, ds+'_val')\n",
    "\ttrain_nodes = getattr(dataCenter, ds+'_train')\n",
    "\tlabels = getattr(dataCenter, ds+'_labels')\n",
    "\n",
    "\tif unsup_loss == 'margin':\n",
    "\t\tnum_neg = 6\n",
    "\telif unsup_loss == 'normal':\n",
    "\t\tnum_neg = 100\n",
    "\telse:\n",
    "\t\tprint(\"unsup_loss can be only 'margin' or 'normal'.\")\n",
    "\t\tsys.exit(1)\n",
    "\n",
    "\ttrain_nodes = shuffle(train_nodes)\n",
    "\n",
    "\tmodels = [graphSage, classification]\n",
    "\tparams = []\n",
    "\tfor model in models:\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\tif param.requires_grad:\n",
    "\t\t\t\tparams.append(param)\n",
    "\n",
    "\toptimizer = torch.optim.SGD(params, lr=0.7)\n",
    "\toptimizer.zero_grad()\n",
    "\tfor model in models:\n",
    "\t\tmodel.zero_grad()\n",
    "\n",
    "\tbatches = math.ceil(len(train_nodes) / b_sz)\n",
    "\n",
    "\tvisited_nodes = set()\n",
    "\tfor index in range(batches):\n",
    "\t\tnodes_batch = train_nodes[index*b_sz:(index+1)*b_sz]\n",
    "\n",
    "\t\t# extend nodes batch for unspervised learning\n",
    "\t\t# no conflicts with supervised learning\n",
    "\t\tnodes_batch = np.asarray(list(unsupervised_loss.extend_nodes(nodes_batch, num_neg=num_neg)))\n",
    "\t\tvisited_nodes |= set(nodes_batch)\n",
    "\n",
    "\t\t# get ground-truth for the nodes batch\n",
    "\t\tlabels_batch = labels[nodes_batch]\n",
    "\n",
    "\t\t# feed nodes batch to the graphSAGE\n",
    "\t\t# returning the nodes embeddings\n",
    "\t\tembs_batch = graphSage(nodes_batch)\n",
    "\n",
    "\t\tif learn_method == 'sup':\n",
    "\t\t\t# superivsed learning\n",
    "\t\t\tlogists = classification(embs_batch)\n",
    "\t\t\tloss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "\t\t\tloss_sup /= len(nodes_batch)\n",
    "\t\t\tloss = loss_sup\n",
    "\t\telif learn_method == 'plus_unsup':\n",
    "\t\t\t# superivsed learning\n",
    "\t\t\tlogists = classification(embs_batch)\n",
    "\t\t\tloss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "\t\t\tloss_sup /= len(nodes_batch)\n",
    "\t\t\t# unsuperivsed learning\n",
    "\t\t\tif unsup_loss == 'margin':\n",
    "\t\t\t\tloss_net = unsupervised_loss.get_loss_margin(embs_batch, nodes_batch)\n",
    "\t\t\telif unsup_loss == 'normal':\n",
    "\t\t\t\tloss_net = unsupervised_loss.get_loss_sage(embs_batch, nodes_batch)\n",
    "\t\t\tloss = loss_sup + loss_net\n",
    "\t\telse:\n",
    "\t\t\tif unsup_loss == 'margin':\n",
    "\t\t\t\tloss_net = unsupervised_loss.get_loss_margin(embs_batch, nodes_batch)\n",
    "\t\t\telif unsup_loss == 'normal':\n",
    "\t\t\t\tloss_net = unsupervised_loss.get_loss_sage(embs_batch, nodes_batch)\n",
    "\t\t\tloss = loss_net\n",
    "\n",
    "\t\tprint('Step [{}/{}], Loss: {:.4f}, Dealed Nodes [{}/{}] '.format(index+1, batches, loss.item(), len(visited_nodes), len(train_nodes)))\n",
    "\t\tloss.backward()\n",
    "\t\tfor model in models:\n",
    "\t\t\tnn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.zero_grad()\n",
    "\n",
    "\treturn graphSage, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSage with Supervised Learning\n",
      "----------------------EPOCH 0-----------------------\n",
      "Step [1/68], Loss: 1.9683, Dealed Nodes [1034/1355] \n",
      "Step [2/68], Loss: 1.9199, Dealed Nodes [1262/1355] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\斋藤飞鸟小男友\\AppData\\Local\\Temp\\ipykernel_85916\\3769236254.py:121: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  neg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes # 从二阶邻居开始采样\n",
      "C:\\Users\\斋藤飞鸟小男友\\AppData\\Local\\Temp\\ipykernel_85916\\1273033684.py:87: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs] # [set(随机采样的邻居集合),set(),set()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [3/68], Loss: 1.8769, Dealed Nodes [1309/1355] \n",
      "Step [4/68], Loss: 1.8552, Dealed Nodes [1339/1355] \n",
      "Step [5/68], Loss: 1.8169, Dealed Nodes [1347/1355] \n",
      "Step [6/68], Loss: 1.7914, Dealed Nodes [1350/1355] \n",
      "Step [7/68], Loss: 1.7642, Dealed Nodes [1353/1355] \n",
      "Step [8/68], Loss: 1.7383, Dealed Nodes [1354/1355] \n",
      "Step [9/68], Loss: 1.7055, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 1.6669, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 1.6499, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 1.6137, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 1.5622, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 1.4962, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 1.4469, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 1.3595, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 1.2960, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 1.1966, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 1.1062, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 1.0341, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.9723, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.9069, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.8296, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.8207, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.7543, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.7811, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.8155, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.9615, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.8871, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.9610, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.5867, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.5360, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.5173, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.4792, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.4591, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.4326, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.4047, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.4034, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.3908, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.3759, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.3724, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.3605, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.3282, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.3201, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.3158, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.3077, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.3121, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.2957, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.2767, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.2823, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.2619, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.2968, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.2785, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.2571, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.2480, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.2546, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.2558, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.2215, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.2322, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.2213, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.2204, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.2396, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.2211, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.2263, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.2207, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.2194, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.2188, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.2203, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 1-----------------------\n",
      "Step [1/68], Loss: 0.1923, Dealed Nodes [1019/1355] \n",
      "Step [2/68], Loss: 0.2211, Dealed Nodes [1262/1355] \n",
      "Step [3/68], Loss: 0.2054, Dealed Nodes [1321/1355] \n",
      "Step [4/68], Loss: 0.1930, Dealed Nodes [1339/1355] \n",
      "Step [5/68], Loss: 0.2149, Dealed Nodes [1347/1355] \n",
      "Step [6/68], Loss: 0.1930, Dealed Nodes [1349/1355] \n",
      "Step [7/68], Loss: 0.2020, Dealed Nodes [1352/1355] \n",
      "Step [8/68], Loss: 0.1994, Dealed Nodes [1354/1355] \n",
      "Step [9/68], Loss: 0.1755, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 0.1864, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.2065, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.1924, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.2028, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.2027, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.2086, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.2159, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.2254, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.2077, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.1730, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.1530, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.1818, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.1874, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.2005, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.1876, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.1829, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.1776, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.1732, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.1759, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.1851, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.1879, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.1739, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.1884, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.1740, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.1833, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.1788, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.1485, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.1543, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.1370, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.1412, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.1371, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.1417, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.1474, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.1293, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.1229, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.1352, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.1389, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.1057, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.1389, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.1376, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.1465, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.1381, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.1366, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.1515, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.1409, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.1700, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.1522, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.1492, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.1319, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.1219, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.1274, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.1325, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.1348, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.1208, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.1112, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.1038, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.1133, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.1076, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.1063, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 2-----------------------\n",
      "Step [1/68], Loss: 0.1116, Dealed Nodes [1032/1355] \n",
      "Step [2/68], Loss: 0.1260, Dealed Nodes [1255/1355] \n",
      "Step [3/68], Loss: 0.1162, Dealed Nodes [1327/1355] \n",
      "Step [4/68], Loss: 0.1139, Dealed Nodes [1341/1355] \n",
      "Step [5/68], Loss: 0.1288, Dealed Nodes [1346/1355] \n",
      "Step [6/68], Loss: 0.1221, Dealed Nodes [1351/1355] \n",
      "Step [7/68], Loss: 0.1238, Dealed Nodes [1351/1355] \n",
      "Step [8/68], Loss: 0.1318, Dealed Nodes [1353/1355] \n",
      "Step [9/68], Loss: 0.1213, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 0.1197, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.1111, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.1133, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.1132, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.1052, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.1087, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0964, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0923, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0891, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0941, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0894, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.1004, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0958, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0887, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0932, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0994, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0872, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0894, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0819, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0742, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0874, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0883, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0834, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0854, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0872, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0940, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0946, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0911, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0937, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0921, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0945, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0893, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0865, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0666, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0827, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0828, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0775, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0760, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0857, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0778, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0823, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0987, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0990, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0944, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0810, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0800, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0779, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0723, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0776, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0969, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0945, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.1094, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.1103, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0947, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0799, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0819, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0899, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0825, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0748, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 3-----------------------\n",
      "Step [1/68], Loss: 0.0776, Dealed Nodes [1030/1355] \n",
      "Step [2/68], Loss: 0.0695, Dealed Nodes [1271/1355] \n",
      "Step [3/68], Loss: 0.0694, Dealed Nodes [1331/1355] \n",
      "Step [4/68], Loss: 0.0675, Dealed Nodes [1343/1355] \n",
      "Step [5/68], Loss: 0.0637, Dealed Nodes [1345/1355] \n",
      "Step [6/68], Loss: 0.0647, Dealed Nodes [1350/1355] \n",
      "Step [7/68], Loss: 0.0731, Dealed Nodes [1354/1355] \n",
      "Step [8/68], Loss: 0.0632, Dealed Nodes [1354/1355] \n",
      "Step [9/68], Loss: 0.0636, Dealed Nodes [1354/1355] \n",
      "Step [10/68], Loss: 0.0809, Dealed Nodes [1354/1355] \n",
      "Step [11/68], Loss: 0.0683, Dealed Nodes [1354/1355] \n",
      "Step [12/68], Loss: 0.0685, Dealed Nodes [1354/1355] \n",
      "Step [13/68], Loss: 0.0645, Dealed Nodes [1354/1355] \n",
      "Step [14/68], Loss: 0.0626, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0568, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0673, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0626, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0600, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0691, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0637, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0730, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0704, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0716, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0627, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0591, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0595, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0551, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0604, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0592, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0626, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0600, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0667, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0617, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0649, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0659, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0608, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0703, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0848, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0847, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.1099, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.1375, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.1819, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.1823, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.2101, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.1595, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0889, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0734, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0643, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0451, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0637, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0573, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0616, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0511, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0573, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0522, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0580, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0454, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0568, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0512, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0579, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0632, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0513, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0508, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0498, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0530, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0551, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0458, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0490, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 4-----------------------\n",
      "Step [1/68], Loss: 0.0496, Dealed Nodes [994/1355] \n",
      "Step [2/68], Loss: 0.0474, Dealed Nodes [1212/1355] \n",
      "Step [3/68], Loss: 0.0512, Dealed Nodes [1315/1355] \n",
      "Step [4/68], Loss: 0.0521, Dealed Nodes [1336/1355] \n",
      "Step [5/68], Loss: 0.0482, Dealed Nodes [1347/1355] \n",
      "Step [6/68], Loss: 0.0517, Dealed Nodes [1349/1355] \n",
      "Step [7/68], Loss: 0.0537, Dealed Nodes [1353/1355] \n",
      "Step [8/68], Loss: 0.0566, Dealed Nodes [1355/1355] \n",
      "Step [9/68], Loss: 0.0490, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 0.0576, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.0507, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.0566, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.0447, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.0479, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0498, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0584, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0476, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0478, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0488, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0504, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0520, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0493, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0449, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0451, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0499, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0455, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0431, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0421, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0407, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0526, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0464, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0404, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0515, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0440, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0486, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0450, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0512, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0409, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0458, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0409, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0447, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0492, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0493, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0397, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0478, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0431, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0433, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0459, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0384, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0359, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0433, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0429, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0395, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0392, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0441, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0418, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0413, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0413, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0427, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0367, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0433, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0440, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0439, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0420, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0446, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0508, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0440, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0514, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 5-----------------------\n",
      "Step [1/68], Loss: 0.0478, Dealed Nodes [994/1355] \n",
      "Step [2/68], Loss: 0.0482, Dealed Nodes [1227/1355] \n",
      "Step [3/68], Loss: 0.0493, Dealed Nodes [1305/1355] \n",
      "Step [4/68], Loss: 0.0448, Dealed Nodes [1326/1355] \n",
      "Step [5/68], Loss: 0.0414, Dealed Nodes [1343/1355] \n",
      "Step [6/68], Loss: 0.0433, Dealed Nodes [1349/1355] \n",
      "Step [7/68], Loss: 0.0402, Dealed Nodes [1353/1355] \n",
      "Step [8/68], Loss: 0.0471, Dealed Nodes [1353/1355] \n",
      "Step [9/68], Loss: 0.0434, Dealed Nodes [1354/1355] \n",
      "Step [10/68], Loss: 0.0396, Dealed Nodes [1354/1355] \n",
      "Step [11/68], Loss: 0.0350, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.0440, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.0417, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.0368, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0415, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0430, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0498, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0356, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0459, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0375, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0465, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0385, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0314, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0359, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0392, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0350, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0369, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0328, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0462, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0357, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0349, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0403, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0321, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0387, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0393, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0369, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0423, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0361, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0334, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0292, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0358, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0377, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0307, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0390, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0368, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0349, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0343, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0406, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0353, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0284, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0354, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0344, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0298, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0279, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0330, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0319, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0376, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0342, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0277, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0300, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0283, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0274, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0322, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0349, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0301, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0398, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0378, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0331, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 6-----------------------\n",
      "Step [1/68], Loss: 0.0339, Dealed Nodes [973/1355] \n",
      "Step [2/68], Loss: 0.0311, Dealed Nodes [1208/1355] \n",
      "Step [3/68], Loss: 0.0340, Dealed Nodes [1309/1355] \n",
      "Step [4/68], Loss: 0.0352, Dealed Nodes [1345/1355] \n",
      "Step [5/68], Loss: 0.0267, Dealed Nodes [1349/1355] \n",
      "Step [6/68], Loss: 0.0331, Dealed Nodes [1350/1355] \n",
      "Step [7/68], Loss: 0.0401, Dealed Nodes [1352/1355] \n",
      "Step [8/68], Loss: 0.0351, Dealed Nodes [1355/1355] \n",
      "Step [9/68], Loss: 0.0407, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 0.0264, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.0304, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.0364, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.0317, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.0308, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0337, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0388, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0299, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0346, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0285, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0270, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0319, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0319, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0315, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0304, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0393, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0295, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0309, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0257, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0339, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0363, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0281, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0309, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0354, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0371, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0327, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0299, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0353, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0325, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0347, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0338, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0301, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0361, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0420, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0291, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0314, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0369, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0310, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0347, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0311, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0294, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0240, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0269, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0305, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0320, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0255, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0318, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0279, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0326, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0249, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0254, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0274, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0273, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0338, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0267, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0280, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0230, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0303, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0236, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 7-----------------------\n",
      "Step [1/68], Loss: 0.0255, Dealed Nodes [1036/1355] \n",
      "Step [2/68], Loss: 0.0249, Dealed Nodes [1234/1355] \n",
      "Step [3/68], Loss: 0.0268, Dealed Nodes [1312/1355] \n",
      "Step [4/68], Loss: 0.0271, Dealed Nodes [1340/1355] \n",
      "Step [5/68], Loss: 0.0231, Dealed Nodes [1345/1355] \n",
      "Step [6/68], Loss: 0.0252, Dealed Nodes [1352/1355] \n",
      "Step [7/68], Loss: 0.0233, Dealed Nodes [1352/1355] \n",
      "Step [8/68], Loss: 0.0283, Dealed Nodes [1354/1355] \n",
      "Step [9/68], Loss: 0.0277, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 0.0206, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.0262, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.0236, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.0273, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.0284, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0232, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0253, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0220, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0299, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0298, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0276, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0282, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0301, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0348, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0241, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0302, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0269, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0238, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0290, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0272, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0281, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0306, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0318, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0304, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0316, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0252, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0271, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0236, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0260, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0218, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0213, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0242, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0208, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0273, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0204, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0293, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0303, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0237, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0231, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0207, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0245, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0189, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0188, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0250, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0269, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0239, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0250, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0267, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0224, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0277, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0202, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0275, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0220, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0276, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0191, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0303, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0213, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0233, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0188, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 8-----------------------\n",
      "Step [1/68], Loss: 0.0200, Dealed Nodes [982/1355] \n",
      "Step [2/68], Loss: 0.0217, Dealed Nodes [1219/1355] \n",
      "Step [3/68], Loss: 0.0221, Dealed Nodes [1314/1355] \n",
      "Step [4/68], Loss: 0.0228, Dealed Nodes [1341/1355] \n",
      "Step [5/68], Loss: 0.0211, Dealed Nodes [1346/1355] \n",
      "Step [6/68], Loss: 0.0185, Dealed Nodes [1350/1355] \n",
      "Step [7/68], Loss: 0.0215, Dealed Nodes [1353/1355] \n",
      "Step [8/68], Loss: 0.0229, Dealed Nodes [1354/1355] \n",
      "Step [9/68], Loss: 0.0262, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 0.0220, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.0251, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.0305, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.0273, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.0213, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0231, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0212, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0238, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0235, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0235, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0297, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0287, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0237, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0251, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0185, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0198, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0220, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0172, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0203, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0279, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0220, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0211, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0216, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0245, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0221, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0163, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0211, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0238, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0244, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0259, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0243, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0223, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0190, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0206, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0185, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0178, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0198, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0229, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0221, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0241, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0215, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0182, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0274, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0206, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0230, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0230, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0245, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0216, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0281, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0265, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0307, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0238, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0197, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0202, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0221, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0250, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0207, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0182, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0252, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "----------------------EPOCH 9-----------------------\n",
      "Step [1/68], Loss: 0.0220, Dealed Nodes [1047/1355] \n",
      "Step [2/68], Loss: 0.0223, Dealed Nodes [1259/1355] \n",
      "Step [3/68], Loss: 0.0196, Dealed Nodes [1318/1355] \n",
      "Step [4/68], Loss: 0.0191, Dealed Nodes [1344/1355] \n",
      "Step [5/68], Loss: 0.0225, Dealed Nodes [1353/1355] \n",
      "Step [6/68], Loss: 0.0204, Dealed Nodes [1354/1355] \n",
      "Step [7/68], Loss: 0.0278, Dealed Nodes [1354/1355] \n",
      "Step [8/68], Loss: 0.0250, Dealed Nodes [1354/1355] \n",
      "Step [9/68], Loss: 0.0231, Dealed Nodes [1354/1355] \n",
      "Step [10/68], Loss: 0.0225, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.0162, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.0226, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.0157, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.0177, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0132, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0207, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0203, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0241, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0183, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0261, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0219, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0206, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0216, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0213, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0198, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0169, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0152, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0226, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0217, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0197, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0174, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0212, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0179, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0209, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0224, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0171, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0189, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0183, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0158, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0190, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0215, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0205, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0133, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0204, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0211, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0177, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0154, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0190, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0153, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0148, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0166, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0203, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0225, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0195, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0173, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0178, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0211, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0170, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0180, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0187, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0167, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0198, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0191, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0132, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0150, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0212, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0203, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0159, Dealed Nodes [1355/1355] \n",
      "GraphSage(\n",
      "  (sage_layer1): SageLayer()\n",
      "  (sage_layer2): SageLayer()\n",
      ") Classification(\n",
      "  (fc1): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n",
      "Validation F1: 0.8403547671840355\n",
      "Test F1: 0.8603104212860311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\斋藤飞鸟小男友\\AppData\\Local\\Temp\\ipykernel_85916\\1273033684.py:87: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs] # [set(随机采样的邻居集合),set(),set()]\n"
     ]
    }
   ],
   "source": [
    "file_paths = {'cora_content':'C:\\d2l\\practice\\GNN\\data\\cora\\cora.content','cora_cite':'C:\\d2l\\practice\\GNN\\data\\cora\\cora.cites'}\n",
    "datacenter  = DataCenter(file_paths)\n",
    "datacenter.load_Dataset()\n",
    "feature_data = torch.FloatTensor(getattr(datacenter, 'cora'+'_feats'))\n",
    "label_data = torch.from_numpy(getattr(datacenter,'cora'+'_labels')).long()\n",
    "adj_lists = getattr(datacenter,'cora'+'_adj_lists')\n",
    "random.seed(824)\n",
    "np.random.seed(824)\n",
    "torch.manual_seed(824)\n",
    "torch.cuda.manual_seed_all(824)\n",
    "learn_method = 'sup'\n",
    "ds = 'cora'\n",
    "epochs = 10\n",
    "max_vali_f1=0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "graphSage = GraphSage(2, feature_data.size(1), 128, feature_data, getattr(datacenter, ds+'_adj_lists'), device, gcn='store_true', agg_func='MEAN')\n",
    "num_labels = len(set(getattr(datacenter, ds+'_labels')))\n",
    "classification = Classification(128, num_labels)\n",
    "unsupervised_loss = UnsupervisedLoss(getattr(datacenter, ds+'_adj_lists'), getattr(datacenter, ds+'_train'), device)\n",
    "if learn_method == 'sup':\n",
    "    print('GraphSage with Supervised Learning')\n",
    "elif learn_method == 'plus_unsup':\n",
    "    print('GraphSage with Supervised Learning plus Net Unsupervised Learning')\n",
    "else:\n",
    "    print('GraphSage with Net Unsupervised Learning')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('----------------------EPOCH %d-----------------------' % epoch)\n",
    "    graphSage, classification = apply_model(datacenter, ds, graphSage, classification, unsupervised_loss, 20, 'normal', device, learn_method)\n",
    "    print(graphSage, classification)\n",
    "    if (epoch+1) % 2 == 0 and learn_method == 'unsup':\n",
    "        classification, max_vali_f1 = train_classification(datacenter, graphSage, classification, ds, device,max_vali_f1, 'debug')\n",
    "if learn_method != 'unsup':\n",
    "\t\tmax_vali_f1 = evaluate(datacenter, ds, graphSage, classification, device, max_vali_f1 , 'debug', epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
